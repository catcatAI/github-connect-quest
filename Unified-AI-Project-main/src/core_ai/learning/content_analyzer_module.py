import uuid # For hsp_fact_id default in process_hsp_fact_content
import spacy
from spacy.tokens import Doc
from typing import List, Dict, Any, Optional, Tuple, TypedDict # Added TypedDict
import networkx as nx

from shared.types.common_types import KGEntity, KGRelationship, KnowledgeGraph
from hsp.types import HSPFactPayload # Import HSPFactPayload

from spacy.matcher import Matcher # Added Matcher

# --- Types for process_hsp_fact_content return value ---
class ProcessedTripleInfo(TypedDict):
    """Detailed information about a semantic triple processed by ContentAnalyzerModule."""
    subject_id: str
    predicate_type: str
    object_id: str
    original_subject_uri: str
    original_predicate_uri: str
    original_object_uri_or_literal: Any
    object_is_uri: bool

class CAHSPFactProcessingResult(TypedDict):
    """Result structure from ContentAnalyzerModule's processing of an HSP fact."""
    updated_graph: bool
    processed_triple: Optional[ProcessedTripleInfo]
# --- End Types ---

# Attempt to load a spaCy model
# For a real application, model management (downloading, versioning) would be more robust.
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    print("Downloading spaCy en_core_web_sm model...")
    spacy.cli.download("en_core_web_sm") # type: ignore
    nlp = spacy.load("en_core_web_sm")

class ContentAnalyzerModule:
    def __init__(self, spacy_model_name: str = "en_core_web_sm"):
        """
        Initializes the ContentAnalyzerModule.
        Tries to load the specified spaCy model and initializes Matcher.
        """
        global nlp # Use the globally loaded model or try to load the specified one
        if spacy_model_name != "en_core_web_sm" or not nlp: # if a different model is requested or global failed
            try:
                nlp = spacy.load(spacy_model_name)
                print(f"Successfully loaded spaCy model: {spacy_model_name}")
            except OSError:
                print(f"spaCy model '{spacy_model_name}' not found. Please download it.")
                print(f"Attempting to download '{spacy_model_name}'...")
                try:
                    spacy.cli.download(spacy_model_name) # type: ignore
                    nlp = spacy.load(spacy_model_name)
                    print(f"Successfully downloaded and loaded spaCy model: {spacy_model_name}")
                except Exception as e:
                    print(f"Failed to download/load {spacy_model_name}. Using en_core_web_sm as fallback if available, else raising error.")
                    if nlp is None and spacy_model_name != "en_core_web_sm": # if global also failed for en_core_web_sm
                        nlp = spacy.load("en_core_web_sm") # try one last time for default
                    elif nlp is None:
                         raise RuntimeError(f"Could not load any spaCy model. Last error: {e}")

import yaml # For loading config
import os   # For path construction

# ... (other imports)

class ContentAnalyzerModule:
    def __init__(self, spacy_model_name: str = "en_core_web_sm", ontology_mapping_filepath: Optional[str] = None):
        """
        Initializes the ContentAnalyzerModule.
        Tries to load the specified spaCy model, ontology mappings, and initializes Matcher.
        """
        global nlp
        if spacy_model_name != "en_core_web_sm" or not nlp:
            try:
                nlp = spacy.load(spacy_model_name)
                print(f"Successfully loaded spaCy model: {spacy_model_name}")
            except OSError:
                print(f"spaCy model '{spacy_model_name}' not found. Attempting to download...")
                try:
                    spacy.cli.download(spacy_model_name) # type: ignore
                    nlp = spacy.load(spacy_model_name)
                    print(f"Successfully downloaded and loaded spaCy model: {spacy_model_name}")
                except Exception as e:
                    print(f"Failed to download/load {spacy_model_name}. Using en_core_web_sm as fallback if available, else raising error.")
                    if nlp is None and spacy_model_name != "en_core_web_sm":
                        nlp = spacy.load("en_core_web_sm")
                    elif nlp is None:
                         raise RuntimeError(f"Could not load any spaCy model. Last error: {e}")

        self.nlp = nlp
        self.matcher = Matcher(self.nlp.vocab)
        self.graph: nx.DiGraph = nx.DiGraph()

        self.internal_uri_prefixes: Dict[str, str] = {}
        self.ontology_mapping: Dict[str, str] = {}
        self._load_ontology_mappings(ontology_mapping_filepath)

        self._initialize_matchers()
        print(f"ContentAnalyzerModule initialized with spaCy model: {self.nlp.meta['name']}. Internal graph created. Ontology mappings loaded: {len(self.ontology_mapping)} entries.")

    def _load_ontology_mappings(self, filepath: Optional[str]):
        if filepath is None:
            # Construct default path: <project_root>/configs/ontology_mappings.yaml
            current_script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.abspath(os.path.join(current_script_dir, "..", "..", ".."))
            filepath = os.path.join(project_root, "configs", "ontology_mappings.yaml")

        if not os.path.exists(filepath):
            print(f"ContentAnalyzerModule: Ontology mapping file not found at '{filepath}'. Using empty mappings.")
            self.internal_uri_prefixes = {"entity_type": "cai_type:", "property": "cai_prop:", "entity_instance": "cai_instance:"} # Default prefixes
            self.ontology_mapping = {}
            return

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                mappings_config = yaml.safe_load(f)

            self.internal_uri_prefixes = mappings_config.get("internal_uri_prefixes",
                {"entity_type": "cai_type:", "property": "cai_prop:", "entity_instance": "cai_instance:"}) # Default if missing

            # Combine class_mappings and property_mappings into a single ontology_mapping dict
            self.ontology_mapping = {}
            self.ontology_mapping.update(mappings_config.get("class_mappings", {}))
            self.ontology_mapping.update(mappings_config.get("property_mappings", {}))
            self.ontology_mapping.update(mappings_config.get("instance_mappings", {})) # If we add instance mappings

            print(f"ContentAnalyzerModule: Loaded {len(self.ontology_mapping)} ontology mappings and "
                  f"{len(self.internal_uri_prefixes)} prefix definitions from '{filepath}'.")

        except yaml.YAMLError as ye:
            print(f"ContentAnalyzerModule: Error parsing YAML from ontology mapping file '{filepath}': {ye}")
            self.ontology_mapping = {} # Fallback to empty
        except Exception as e:
            print(f"ContentAnalyzerModule: Error loading ontology mapping file '{filepath}': {e}")
            self.ontology_mapping = {} # Fallback to empty


    def _initialize_matchers(self):
        """Initializes spaCy Matcher patterns."""
        # Pattern for "ORG located in GPE" / "ORG based in GPE"
        # Example: "Apple Inc. is located in Cupertino."
        #          "Microsoft, based in Redmond, ..."
        pattern_located_in = [
            {"ENT_TYPE": "ORG"},
            {"LEMMA": "be", "OP": "?"},  # e.g., "is" / "was"
            {"LEMMA": {"IN": ["locate", "base"]}, "OP": "?"}, # e.g., "located" / "based"
            # Handling cases like "Microsoft, based in Redmond" - no "is"
            # This requires more complex patterns or dependency parsing.
            # For now, let's assume "is/was" might be present before "located/based".
            # A more flexible pattern might have more optional parts or use lists of patterns.
            # Consider also: ORG, based in GPE (comma, participle)
            # For "Microsoft, based in Redmond", "based" is a VERB here.
            # The pattern might need to be: ORG [is/was]? [located/based]? in GPE
            # ORG [,]? based in GPE  -- this is harder with token-based matcher alone for syntax like appositives.
            {"IS_PUNCT": True, "OP": "?"}, # Optional comma if "based" follows directly
            {"LEMMA": "in"},
            {"ENT_TYPE": {"IN": ["GPE", "LOC"]}}
        ]
        self.matcher.add("LOCATED_IN", [pattern_located_in])

        # Pattern for "PERSON works for ORG"
        # Example: "Tim Cook works for Apple."
        #          "Satya Nadella, CEO of Microsoft, works at Microsoft." (at might be harder)
        pattern_works_for = [
            {"ENT_TYPE": "PERSON"},
            {"LEMMA": "work"},
            {"LEMMA": "for"}, # Could also be "at" but "at" is more ambiguous
            {"ENT_TYPE": "ORG"}
        ]
        self.matcher.add("WORKS_FOR", [pattern_works_for])

        # Pattern for "PERSON is TITLE of ORG"
        # Example: "Satya Nadella is CEO of Microsoft."
        titles = [
            "ceo", "president", "founder", "manager", "director", "cto", "coo",
            "cfo", "cio", "cmo", "vp", "chairman", "chairwoman", "chairperson"
        ]
        pattern_person_is_title_of_org = [
            {"ENT_TYPE": "PERSON"},
            {"LEMMA": "be"},
            {"LOWER": {"IN": ["the", "a"]}, "OP": "?"}, # Optional: "the" or "a"
            {"LOWER": {"IN": titles}}, # The title
            {"LOWER": "of"},
            {"ENT_TYPE": "ORG"}
        ]
        self.matcher.add("PERSON_IS_TITLE_OF_ORG", [pattern_person_is_title_of_org])


    def _extract_entities(self, doc: Doc) -> Dict[str, KGEntity]:
        """
        Extracts named entities from a spaCy Doc object.
        Uses entity text as a simple ID for this prototype.
        """
        entities: Dict[str, KGEntity] = {}
        # if "Google develops Android" in doc.text: # Specific debug for test_04
        #     print(f"DEBUG_TEST_04: doc.text = {doc.text}")
        #     print(f"DEBUG_TEST_04: doc.ents = {[(ent.text, ent.label_) for ent in doc.ents]}")

        # Standard NER entities
        for i, ent in enumerate(doc.ents):
            entity_id = f"ent_{ent.text.lower().replace(' ', '_')}_{ent.label_}_{i}"
            if entity_id not in entities: # Should always be true with unique `i`
                entities[entity_id] = {
                    "id": entity_id,
                    "label": ent.text,
                    "type": ent.label_,
                    "attributes": {
                        "start_char": ent.start_char,
                        "end_char": ent.end_char,
                    }
                }

        # Rule-based augmentation for specific known entities if NER misses them
        # Example for "Google"
        # This is a simple heuristic; a more robust solution would use Matcher patterns added to self.nlp or similar.
        google_mentions = [token for token in doc if token.text.lower() == "google"]
        is_google_already_org_entity = any(
            ent.label_ == "ORG" and "google" in ent.text.lower() for ent in doc.ents
        )

        if google_mentions and not is_google_already_org_entity:
            for i, token in enumerate(google_mentions):
                # Check if this token is already part of ANY extracted entity to avoid conflicts
                part_of_other_entity = False
                for ent_id_key in entities:
                    ent_detail = entities[ent_id_key]
                    if token.idx >= ent_detail["attributes"]["start_char"] and \
                       (token.idx + len(token.text)) <= ent_detail["attributes"]["end_char"]:
                        part_of_other_entity = True
                        break
                if not part_of_other_entity:
                    entity_id = f"ent_google_ORG_rule_{i}" # Unique ID for rule-based Google
                    entities[entity_id] = {
                        "id": entity_id,
                        "label": token.text, # Preserve original casing
                        "type": "ORG",
                        "attributes": {
                            "start_char": token.idx,
                            "end_char": token.idx + len(token.text),
                            "rule_added": "heuristic_google_ORG"
                        }
                    }
                    # print(f"DEBUG: Added 'Google' as ORG by rule: {entity_id}") # Removed debug print

        return entities

    def _extract_relationships_prototype(
        self, doc: Doc, extracted_entities: Dict[str, KGEntity]
    ) -> List[KGRelationship]:
        """
        Extracts relationships using dependency parsing and other heuristic patterns.
        This method will also call matcher-based extraction.
        """
        relationships: List[KGRelationship] = []
        entity_id_by_char_offset: Dict[Tuple[int, int], str] = {
            (details["attributes"]["start_char"], details["attributes"]["end_char"]): entity_id
            for entity_id, details in extracted_entities.items()
        }

        def get_entity_id_for_token(token: spacy.tokens.Token) -> Optional[str]:
            """Helper to find if a token is part of any known entity span."""
            for (start_char, end_char), entity_id in entity_id_by_char_offset.items():
                if token.idx >= start_char and (token.idx + len(token.text)) <= end_char:
                    # Check if the token is the head of the entity span or a significant part
                    ent_span = doc.char_span(start_char, end_char)
                    if ent_span and (token == ent_span.root or token in ent_span): # token is the head or part of the entity span
                        return entity_id
            # If token itself isn't the start of an entity, check if its head is, common for multi-word entities
            if token.head != token: # Avoid infinite loop for root token
                for (start_char, end_char), entity_id in entity_id_by_char_offset.items():
                    if token.head.idx >= start_char and (token.head.idx + len(token.head.text)) <= end_char:
                         ent_span = doc.char_span(start_char, end_char)
                         if ent_span and (token.head == ent_span.root or token.head in ent_span):
                            return entity_id
            return None

        for token in doc:
            # 1. SVO (Subject-Verb-Object) and "is_a" (Subject-Verb-Attribute) Pattern
            if token.pos_ in ("VERB", "AUX"): # Changed to include AUX for "is", "was", etc.
                verb_lemma = token.lemma_

                if doc.text == "Google develops Android.": # Specific debug for test_04
                    print(f"DEBUG_TEST_04: Processing verb '{token.text}' (lemma: {verb_lemma})")

                subjects = [child for child in token.children if child.dep_ in ("nsubj", "nsubjpass")]

                # Handle direct objects (dobj) and attributes (attr) for 'is a' type relations
                # Also handle predicative complements (pcomp) if they are nominal
                objects_or_attrs = [child for child in token.children if child.dep_ in ("dobj", "attr")]
                # For verbs like "named", "called", "considered" check for 'oprd' (object predicate)
                objects_or_attrs.extend([child for child in token.children if child.dep_ == "oprd"])


                for subj_token_candidate in subjects:
                    if doc.text == "Google develops Android.": # Specific debug for test_04
                        print(f"DEBUG_TEST_04: SVO subject candidate: '{subj_token_candidate.text}'")
                    subj_entity_id = get_entity_id_for_token(subj_token_candidate)
                    if doc.text == "Google develops Android.": # Specific debug for test_04
                        print(f"DEBUG_TEST_04: SVO subj_entity_id for '{subj_token_candidate.text}': {subj_entity_id}")

                    if subj_entity_id:
                        for obj_attr_token_candidate in objects_or_attrs:
                            obj_attr_entity_id = get_entity_id_for_token(obj_attr_token_candidate)

                            # If the object/attribute is an entity
                            if obj_attr_entity_id and subj_entity_id != obj_attr_entity_id:
                                rel_type = verb_lemma
                                pattern_type = "SVO"
                                if token.lemma_ == "be" and obj_attr_token_candidate.dep_ == "attr":
                                    rel_type = "is_a"
                                    pattern_type = "is_a_attr"
                                # (Removed passive Y is X check as it's usually covered)
                                relationships.append({
                                    "source_id": subj_entity_id,
                                    "target_id": obj_attr_entity_id,
                                    "type": rel_type,
                                    "weight": 0.65 if pattern_type == "is_a_attr" else 0.6,
                                    "attributes": {"pattern": pattern_type, "trigger_token": token.text}
                                })
                            # If the object/attribute is NOT an entity, but could be a descriptive attribute for "is_a"
                            elif token.lemma_ == "be" and obj_attr_token_candidate.dep_ == "attr" and not obj_attr_entity_id:
                                if obj_attr_token_candidate.pos_ == "NOUN":
                                    concept_lemma = obj_attr_token_candidate.lemma_.lower().replace(' ', '_')
                                    concept_id = f"concept_{concept_lemma}"
                                    concept_label = obj_attr_token_candidate.lemma_
                                    if concept_id not in extracted_entities:
                                        extracted_entities[concept_id] = {
                                            "id": concept_id,
                                            "label": concept_label,
                                            "type": "CONCEPT",
                                            "attributes": {"is_conceptual": True, "source_text": obj_attr_token_candidate.text}
                                        }

                                    relationships.append({
                                        "source_id": subj_entity_id,
                                        "target_id": concept_id,
                                        "type": "is_a",
                                        "weight": 0.65,
                                        "attributes": {"pattern": "is_a_concept", "trigger_token": token.text}
                                    })


            # 2. Prepositional Phrase Attachment (Verb-Preposition-Object_Entity)
            #    Example: "cat sat on the mat" -> cat -(sat_on)-> mat
            #    Example: "Apple was founded by Steve Jobs" -> Apple -(founded_by)-> Steve Jobs (passive)
            if token.pos_ == "ADP": # Preposition
                prep_lemma = token.lemma_
                head_verb_token = token.head

                if head_verb_token.pos_ == "VERB":
                    verb_lemma = head_verb_token.lemma_
                    rel_type = f"{verb_lemma}_{prep_lemma}"

                    # Subject of the verb
                    verb_subjects = [child for child in head_verb_token.children if child.dep_ in ("nsubj", "nsubjpass")]
                    # Object of the preposition
                    prep_objects = [child for child in token.children if child.dep_ == "pobj"]

                    for subj_token in verb_subjects:
                        subj_entity_id = get_entity_id_for_token(subj_token)
                        if not subj_entity_id: subj_entity_id = get_entity_id_for_token(subj_token.head)

                        if subj_entity_id:
                            for pobj_token in prep_objects:
                                pobj_entity_id = get_entity_id_for_token(pobj_token)
                                if not pobj_entity_id: pobj_entity_id = get_entity_id_for_token(pobj_token.head)

                                if pobj_entity_id and subj_entity_id != pobj_entity_id:
                                    relationships.append({
                                        "source_id": subj_entity_id,
                                        "target_id": pobj_entity_id,
                                        "type": rel_type,
                                        "weight": 0.5,
                                        "attributes": {"pattern": "Verb_Prep_Obj", "trigger_token": token.text}
                                    })

            # 3. Noun-Preposition-Noun_Entity (e.g., "capital of France", "CEO of Microsoft")
            #    This pattern attempts to extract relationships like has_attribute, part_of, or specific roles.
            if token.pos_ == "ADP":  # Current token is a preposition
                prep_lemma = token.lemma_
                prep_object_token = None
                for child in token.children:
                    if child.dep_ == "pobj":
                        prep_object_token = child
                        break

                if prep_object_token:
                    # Head of the preposition is often the first noun phrase (X in "X prep Y")
                    head_of_prep_phrase_token = token.head # This is X in "X of Y"

                    # Get/Create CONCEPT node for X if it's a NOUN and not an entity
                    source_node_id = get_entity_id_for_token(head_of_prep_phrase_token)
                    source_is_concept = False
                    if not source_node_id and head_of_prep_phrase_token.pos_ == "NOUN":
                        concept_lemma = head_of_prep_phrase_token.lemma_.lower().replace(' ', '_')
                        source_node_id = f"concept_{concept_lemma}"
                        if source_node_id not in extracted_entities:
                            extracted_entities[source_node_id] = {
                                "id": source_node_id, "label": head_of_prep_phrase_token.lemma_, "type": "CONCEPT",
                                "attributes": {"is_conceptual": True, "source_text": head_of_prep_phrase_token.text}
                            }
                        source_is_concept = True

                    target_entity_id = get_entity_id_for_token(prep_object_token) # This is Y

                    if source_node_id and target_entity_id and source_node_id != target_entity_id:
                        rel_type = f"{head_of_prep_phrase_token.lemma_}_{prep_lemma}"
                        weight = 0.4
                        pattern = "Noun_Prep_Noun_General"

                        if prep_lemma.lower() == "of":
                            pattern = "Noun_Prep_Noun_Of"
                            weight = 0.45
                            source_details = extracted_entities.get(source_node_id)
                            target_details = extracted_entities.get(target_entity_id)

                            is_source_significant_entity = source_details and source_details.get("type") in ("ORG", "GPE", "PERSON")
                            if target_details and target_details.get("type") in ("ORG", "GPE", "PERSON") and \
                               (source_is_concept or not is_source_significant_entity):
                                rel_type = f"has_{head_of_prep_phrase_token.lemma_}"
                                relationships.append({
                                    "source_id": target_entity_id,
                                    "target_id": source_node_id,
                                    "type": rel_type, "weight": weight,
                                    "attributes": {"pattern": pattern, "trigger_token": token.text, "original_X": head_of_prep_phrase_token.text, "original_Y": prep_object_token.text }
                                })
                                continue
                            else:
                                rel_type = f"{head_of_prep_phrase_token.lemma_}_of"

                        relationships.append({
                            "source_id": source_node_id, "target_id": target_entity_id,
                            "type": rel_type, "weight": weight,
                            "attributes": {"pattern": pattern, "trigger_token": token.text}
                        })

            # 4. Possessive Relationships (e.g., "Google's CEO", "John's car")
            if token.tag_ == "POS": # Possessive marker like 's
                possessor_token = token.head # The entity that possesses something
                # The thing possessed is often the head of the possessor_token if 's is attached to it
                # Or, it could be a child of the possessor_token's head if the structure is more complex.
                # For "Google's CEO is Sundar", "CEO" is head of "Google", and "is" is head of "CEO".
                # "Google" -> possessor_token.head
                # "CEO" -> token.head (which is "Google") then its head ("CEO")

                # Let's simplify: the head of 's is the possessor.
                # The head of the possessor noun is often the possessed item in simple cases like "Google's CEO"
                # where "CEO" is head of "Google" (which is head of 's)

                possessor_entity_id = get_entity_id_for_token(possessor_token)

                if possessor_entity_id:
                    # The possessed item is typically the head of the possessor noun phrase.
                    # Example: "Google's algorithm": 's head is Google. Google's head is algorithm.
                    # Example: "John's house": 's head is John. John's head is house.
                    # This can be tricky as the actual "possessed" item might be further up.
                    # A common pattern is that the head of the possessor_token (NP's head) is the possessed item.

                    # Let's find what the possessor_token (e.g., "Google") is part of, and find its head.
                    # The token itself ('s) is attached to the possessor. The head of 's is the possessor.
                    # The head of the possessor_token is what we are looking for as the possessed item.

                    possessed_candidate_token = possessor_token.head
                    if possessed_candidate_token == token: # Avoid 's being its own head's possessed item
                         # This happens if 's is attached to something that is the root or has no clear possessed noun phrase head.
                         # e.g. "It's nice" - 's head is It, It's head is is.
                         # We need to look for a noun phrase that this possessive modifies.
                         # Often, the noun following the possessive structure is the possessed item.
                         # "Google's algorithm" -> algorithm is possessor_token.head
                         # "The company's new CEO" -> CEO is possessor_token.head.head (company -> new -> CEO)
                         # This needs a more robust way to find the "possessed noun phrase head"
                         # For now, we'll check immediate children of the possessor_token's head that are nouns
                         # This is still very heuristic.
                        pass


                    # A simpler heuristic: the noun immediately following the 's construction if it's not the verb
                    # Or the head of the possessor token.
                    # Let's try to find the noun phrase that the possessive 's modifies.
                    # The head of the token 's' is the noun it's attached to (e.g., Google).
                    # The actual possessed thing is often the head of that noun, or a sibling.

                    # Consider the noun phrase the possessor is part of.
                    # The head of that noun phrase is often the possessed thing.
                    # E.g. "[[Google]'s [CEO]]" -> CEO is head of Google.
                    # E.g. "[The [old man]'s [dog]]" -> dog is head of man.

                    # The token.head is the word 's is attached to (e.g. Google).
                    # The head of that word (Google.head) is often the possessed item (e.g. CEO).

                    possessed_item_token = possessor_token.head # Head of the word 's is attached to
                                                                # e.g. in "Google's CEO", 's head is Google, Google's head is CEO.
                                                                # e.g. in "The cat's toy", 's head is cat, cat's head is toy.

                    possessed_entity_id = get_entity_id_for_token(possessed_item_token)

                    if possessed_entity_id and possessor_entity_id != possessed_entity_id:
                        relationships.append({
                            "source_id": possessor_entity_id,
                            "target_id": possessed_entity_id,
                            "type": "has_poss_attr", # Generic possessive attribute
                            "weight": 0.5,
                            "attributes": {"pattern": "Possessive", "trigger_token": possessor_token.text + token.text}
                        })
                    # If possessed_item_token is not an entity, but a noun, create a concept node and link to it.
                    elif possessed_item_token.pos_ == "NOUN" and not possessed_entity_id:
                        concept_lemma = possessed_item_token.lemma_.lower().replace(' ', '_')
                        concept_id = f"concept_{concept_lemma}"
                        concept_label = possessed_item_token.lemma_

                        if concept_id not in extracted_entities:
                            extracted_entities[concept_id] = { # Modifying the dict directly
                                "id": concept_id,
                                "label": concept_label,
                                "type": "CONCEPT",
                                "attributes": {"is_conceptual": True, "source_text": possessed_item_token.text}
                            }

                        relationships.append({
                            "source_id": possessor_entity_id,
                            "target_id": concept_id,
                            "type": f"has_{concept_lemma}", # e.g., has_revenue
                            "weight": 0.4,
                            "attributes": {"pattern": "Possessive_AttrToConcept", "trigger_token": possessor_token.text + token.text}
                        })

        # Add relationships from Matcher
        # Note: extracted_entities might have been modified by the possessive logic to include new concepts
        matcher_rels = self._extract_relationships_with_matcher(doc, extracted_entities)
        relationships.extend(matcher_rels)

        # Deduplicate relationships (simple way: based on source, target, type)
        # A more robust deduplication might involve checking attributes or context.
        final_relationships = []
        seen_rels = set()
        for rel in relationships:
            # Normalize by sorting IDs if relationship is symmetric or direction is uncertain for some types
            # For now, assume directionality from extraction is intended.
            rel_tuple = (rel["source_id"], rel["target_id"], rel["type"])
            if rel_tuple not in seen_rels:
                final_relationships.append(rel)
                seen_rels.add(rel_tuple)

        return final_relationships

    def _extract_relationships_with_matcher(self, doc: Doc, extracted_entities: Dict[str, KGEntity]) -> List[KGRelationship]:
        """Extracts relationships using pre-defined spaCy Matcher patterns."""
        if "Innovate Corp is located in Silicon Valley" in doc.text:
            print(f"DEBUG_MATCHER_ENTRY: doc='{doc.text}', entities for matcher: {[(ent.text, ent.label_) for ent in doc.ents]}")
            print(f"DEBUG_MATCHER_ENTRY: extracted_entities keys: {list(extracted_entities.keys())}")

        matches = self.matcher(doc)
        if "Innovate Corp is located in Silicon Valley" in doc.text:
            print(f"DEBUG_MATCHER_ENTRY: Number of raw matches from self.matcher(doc): {len(matches)}")

        found_relationships: List[KGRelationship] = []

        # Helper to find the entity ID for a token if that token is part of a known entity.
        # This is crucial for mapping matcher tokens (which are part of entities) back to our KGEntity IDs.
        def get_entity_id_for_token_in_match(token_in_match: spacy.tokens.Token) -> Optional[str]:
            for ent_id, ent_details in extracted_entities.items():
                # Check if the token's character span is within the entity's character span
                # and that the entity label matches the token's entity type (if set by NER)
                # This ensures we are linking the correct part of a multi-token entity.
                if token_in_match.idx >= ent_details["attributes"]["start_char"] and \
                   (token_in_match.idx + len(token_in_match.text)) <= ent_details["attributes"]["end_char"]:
                    # Additional check: if token has an ent_type_, it should match the entity's type
                    if token_in_match.ent_type_ and token_in_match.ent_type_ == ent_details["type"]:
                        return ent_id
                    elif not token_in_match.ent_type_: # Token itself not marked as ENT by NER, but falls within a larger ENT span
                        return ent_id
            return None


        for match_id, start_token_idx, end_token_idx in matches:
            rule_id_str = self.nlp.vocab.strings[match_id]
            span = doc[start_token_idx:end_token_idx] # The full span of the match

            source_entity_id: Optional[str] = None
            target_entity_id: Optional[str] = None
            rel_type: Optional[str] = None
            weight: float = 0.7 # Default weight for matcher rules

            if rule_id_str == "LOCATED_IN":
                # Pattern: [{"ENT_TYPE": "ORG"}, VERBS?, "in", {"ENT_TYPE": "GPE"}]
                # Iterate through tokens in `span` to find the ORG and GPE based on ENT_TYPE
                org_token, gpe_token = None, None
                for token_in_span in span:
                    if token_in_span.ent_type_ == "ORG" and not org_token:
                        org_token = token_in_span
                    elif token_in_span.ent_type_ in ["GPE", "LOC"]: # last GPE or LOC in span
                        gpe_token = token_in_span # Variable name can remain gpe_token for simplicity

                if org_token and gpe_token:
                    print(f"DEBUG_MATCHER_LOC: Found ORG token '{org_token.text}', GPE/LOC token '{gpe_token.text}' in span '{span.text}'")
                    source_entity_id = get_entity_id_for_token_in_match(org_token)
                    target_entity_id = get_entity_id_for_token_in_match(gpe_token)
                    print(f"DEBUG_MATCHER_LOC: Source ID: {source_entity_id}, Target ID: {target_entity_id}")
                    rel_type = "located_in"

            elif rule_id_str == "WORKS_FOR":
                # Pattern: [{"ENT_TYPE": "PERSON"}, "works", "for", {"ENT_TYPE": "ORG"}]
                person_token, org_token = None, None
                for token_in_span in span:
                    if token_in_span.ent_type_ == "PERSON" and not person_token:
                        person_token = token_in_span
                    elif token_in_span.ent_type_ == "ORG":
                        org_token = token_in_span

                if person_token and org_token:
                    source_entity_id = get_entity_id_for_token_in_match(person_token)
                    target_entity_id = get_entity_id_for_token_in_match(org_token)
                    rel_type = "works_for"
                    weight = 0.75

            elif rule_id_str == "PERSON_IS_TITLE_OF_ORG":
                # Pattern: PERSON - be - TITLE - of - ORG
                person_token, title_token_text, org_token = None, None, None

                # Pattern: PERSON - be - (the|a)? - TITLE - of - ORG
                # Indices:   0      1        2         3      4      5   (if determiner present)
                #            0      1                  2      3      4   (if determiner absent)

                person_token = span[0] # Should always be PERSON if rule matched

                title_idx = 2
                org_idx = 4
                if span[2].lower_ in ["the", "a"]: # Determiner is present
                    title_idx = 3
                    org_idx = 5

                # Check if span is long enough for the determined indices
                if org_idx < len(span): # Ensure all expected tokens are within the span
                    title_token_text = span[title_idx].lemma_.lower()
                    org_token = span[org_idx]

                    # Additional check for entity types (matcher might match tokens, but we need entities)
                    if person_token.ent_type_ != "PERSON" or org_token.ent_type_ != "ORG":
                        # This case should ideally not happen if the pattern is specific enough with ENT_TYPE,
                        # but good for robustness if tokens matched but weren't correctly ID'd as entities by NER prior to matcher.
                        # Or if matcher itself is less strict on ENT_TYPE for some reason.
                        # For now, we rely on the pattern's ENT_TYPE constraints.
                        pass # Consider logging a warning if this happens.

                if person_token and title_token_text and org_token and person_token.ent_type_ == "PERSON" and org_token.ent_type_ == "ORG":
                    # Relationship: ORG --has_<title>--> PERSON
                    source_entity_id = get_entity_id_for_token_in_match(org_token) # ORG is source
                    target_entity_id = get_entity_id_for_token_in_match(person_token) # PERSON is target
                    rel_type = f"has_{title_token_text}"
                    weight = 0.8 # High confidence for this specific pattern


            if source_entity_id and target_entity_id and rel_type and source_entity_id != target_entity_id:
                found_relationships.append({
                    "source_id": source_entity_id,
                    "target_id": target_entity_id,
                    "type": rel_type,
                    "weight": weight,
                    "attributes": {"pattern": rule_id_str, "trigger_text": span.text}
                })
        return found_relationships


    def analyze_content(self, text_content: str) -> Tuple[KnowledgeGraph, nx.DiGraph]:
        """
        Analyzes text content to extract entities and relationships.
        Constructs both a KnowledgeGraph TypedDict and a NetworkX DiGraph.
        Returns both the TypedDict and the NetworkX DiGraph.
        """
        if not self.nlp:
            raise RuntimeError("spaCy NLP model not loaded in ContentAnalyzerModule.")

        doc = self.nlp(text_content)

        extracted_entities_dict = self._extract_entities(doc)
        extracted_relationships_list = self._extract_relationships_prototype(doc, extracted_entities_dict)

        knowledge_graph_data: KnowledgeGraph = {
            "entities": extracted_entities_dict,
            "relationships": extracted_relationships_list,
            "metadata": {
                "source_text_length": len(text_content),
                "processed_with_model": self.nlp.meta["name"],
                "entity_count": len(extracted_entities_dict),
                "relationship_count": len(extracted_relationships_list)
            }
        }

        # Build NetworkX graph
        nx_graph = nx.DiGraph()
        for entity_id, entity_data in knowledge_graph_data["entities"].items():
            # Node attributes should not include the 'id' itself if id is the node identifier
            attributes_for_node = {k: v for k, v in entity_data.items() if k != 'id'}
            nx_graph.add_node(entity_id, **attributes_for_node)

        for rel_data in knowledge_graph_data["relationships"]:
            # Edge attributes should not include source_id and target_id
            attributes_for_edge = {k: v for k, v in rel_data.items() if k not in ('source_id', 'target_id')}
            nx_graph.add_edge(
                rel_data["source_id"],
                rel_data["target_id"],
                **attributes_for_edge
            )

        print(f"NetworkX graph constructed: {nx_graph.number_of_nodes()} nodes, {nx_graph.number_of_edges()} edges.")
        return knowledge_graph_data, nx_graph

    def process_hsp_fact_content(self, hsp_fact_payload: HSPFactPayload, source_ai_id: str) -> CAHSPFactProcessingResult:
        """
        Processes the content of an HSPFactPayload and integrates it into the module's knowledge graph.
        If `statement_nl` exists, it re-analyzes it.
        If `statement_structured` (as semantic triple) exists, it tries to add it directly to the graph,
        applying ontology mappings if configured.
        Updates `self.graph` directly.

        Args:
            hsp_fact_payload: The HSPFactPayload object.
            source_ai_id: The ID of the AI that sent/originated this fact via HSP.

        Returns:
            CAHSPFactProcessingResult: A dictionary indicating if the graph was updated and
                                       details of any processed semantic triple.
        """
        fact_id_str = hsp_fact_payload.get('id', "unknown_hsp_fact_id")
        print(f"ContentAnalyzerModule: Processing HSP Fact ID '{fact_id_str}' from '{source_ai_id}'")

        result: CAHSPFactProcessingResult = {"updated_graph": False, "processed_triple": None}
        confidence = hsp_fact_payload.get('confidence_score', 0.7)
        # Use the provided fact ID or generate one if missing (though HSPFactPayload 'id' is typed as required)
        hsp_fact_internal_ref_id = fact_id_str if fact_id_str != "unknown_hsp_fact_id" else f"hsp_fact_gen_{uuid.uuid4().hex[:6]}"

        # Option 1: Process natural language statement if available
        statement_nl = hsp_fact_payload.get('statement_nl')
        if statement_nl:
            print(f"ContentAnalyzerModule: Analyzing NL statement from HSP fact: \"{statement_nl[:100]}...\"")
            # analyze_content returns a new graph; we need to merge it.
            kg_data_from_nl, nx_graph_from_nl = self.analyze_content(statement_nl)

            if nx_graph_from_nl.number_of_nodes() > 0:
                # Merge nodes and edges from nx_graph_from_nl into self.graph
                # For simplicity, this is a basic union. More sophisticated merging would handle conflicts,
                # update existing node attributes based on confidence/source, etc.
                for node, data in nx_graph_from_nl.nodes(data=True):
                    if node not in self.graph:
                        self.graph.add_node(node, **data)
                        # Add HSP source info to new nodes from this fact
                        self.graph.nodes[node]['hsp_source_info'] = {'origin_fact_id': hsp_fact_internal_ref_id, 'source_ai': source_ai_id, 'confidence': confidence}
                        result["updated_graph"] = True
                    else:
                        # Node exists, maybe update confidence or add as another source? For now, just log.
                        print(f"ContentAnalyzerModule: Node '{node}' from HSP fact NL already exists. Not overwriting attributes.")

                for u, v, data in nx_graph_from_nl.edges(data=True):
                    if not self.graph.has_edge(u,v) or self.graph.edges[u,v].get('type') != data.get('type'): # type: ignore
                        self.graph.add_edge(u, v, **data)
                        # Add HSP source info to new edges
                        self.graph.edges[u,v]['hsp_source_info'] = {'origin_fact_id': hsp_fact_internal_ref_id, 'source_ai': source_ai_id, 'confidence': confidence} # type: ignore
                        result["updated_graph"] = True
                    else:
                        # Edge exists, maybe update weight/confidence? For now, just log.
                        print(f"ContentAnalyzerModule: Edge '{u}'->'{v}' (type {data.get('type')}) from HSP fact NL already exists.")

                if result["updated_graph"]:
                    print(f"ContentAnalyzerModule: Merged {nx_graph_from_nl.number_of_nodes()} nodes and {nx_graph_from_nl.number_of_edges()} edges from NL analysis of HSP fact into main graph.")

        # Option 2: Process structured statement (e.g., semantic triple)
        # This part is more conceptual for now and needs careful implementation of "deep mapping".
        statement_structured = hsp_fact_payload.get('statement_structured')
        statement_type = hsp_fact_payload.get('statement_type')

        if statement_type == "semantic_triple" and isinstance(statement_structured, dict):
            subj_uri = statement_structured.get('subject_uri')
            pred_uri = statement_structured.get('predicate_uri')
            obj_data = statement_structured.get('object_literal') or statement_structured.get('object_uri')
            obj_is_uri = bool(statement_structured.get('object_uri'))

            if subj_uri and pred_uri and obj_data:
                print(f"ContentAnalyzerModule: Processing semantic triple from HSP fact: {subj_uri} - {pred_uri} - {obj_data}")

                # Attempt to map URIs to internal representations
                s_original_uri = subj_uri
                p_original_uri = pred_uri
                o_original_uri = obj_data if obj_is_uri else None

                s_id = self.ontology_mapping.get(s_original_uri, s_original_uri)
                p_type = self.ontology_mapping.get(p_original_uri, p_original_uri.split('/')[-1].split('#')[-1]) # Mapped or derived

                o_id: str
                o_label: str
                o_type: str

                if obj_is_uri and o_original_uri:
                    o_id = self.ontology_mapping.get(o_original_uri, o_original_uri)
                    o_label = o_original_uri.split('/')[-1].split('#')[-1] # Use fragment for label initially
                    o_type = "HSP_URI_Entity" # Default, could be refined if o_id is a mapped type like cai_type:City
                    if o_id.startswith(self.internal_uri_prefixes["entity_type"]):
                        o_type = o_id # e.g. cai_type:City
                else: # Literal object
                    o_literal_val_str = str(obj_data)
                    o_id = f"literal_{o_literal_val_str.lower().replace(' ','_')[:30]}_{uuid.uuid4().hex[:4]}" # Make literal IDs more unique
                    o_label = o_literal_val_str
                    o_type = statement_structured.get('object_datatype', "LITERAL_VALUE") # Use datatype if provided

                s_label = s_original_uri.split('/')[-1].split('#')[-1]
                s_type = "HSP_URI_Entity"
                if s_id.startswith(self.internal_uri_prefixes["entity_type"]):
                    s_type = s_id

                initial_node_count = self.graph.number_of_nodes()
                initial_edge_count = self.graph.number_of_edges()

                # Add/update subject node
                if not self.graph.has_node(s_id):
                    self.graph.add_node(s_id, label=s_label, type=s_type, original_uri=s_original_uri,
                                        hsp_source_info={'origin_fact_id': hsp_fact_internal_ref_id, 'source_ai': source_ai_id, 'confidence': confidence})

                # Add/update object node
                if not self.graph.has_node(o_id):
                    node_attrs: Dict[str, Any] = {"label": o_label, "type": o_type,
                                  "hsp_source_info": {'origin_fact_id': hsp_fact_internal_ref_id, 'source_ai': source_ai_id, 'confidence': confidence}}
                    if o_original_uri: # Store original URI if it was a URI
                        node_attrs["original_uri"] = o_original_uri
                    self.graph.add_node(o_id, **node_attrs)

                # Add relationship using mapped predicate type
                edge_added_or_updated = False
                if not self.graph.has_edge(s_id, o_id) or self.graph.edges[s_id, o_id].get('type') != p_type: # type: ignore
                    self.graph.add_edge(s_id, o_id, type=p_type, original_predicate_uri=p_original_uri,
                                        weight=confidence, hsp_source_info={'origin_fact_id': hsp_fact_internal_ref_id, 'source_ai': source_ai_id})
                    edge_added_or_updated = True
                    print(f"ContentAnalyzerModule: Added/updated edge from mapped semantic triple: {s_id} -[{p_type}]-> {o_id}")
                else:
                     # Check if predicate URI is different, even if mapped p_type is same. This is less likely with current mapping.
                     # Or if confidence is higher, update edge attributes.
                     existing_edge_data = self.graph.edges[s_id, o_id] # type: ignore
                     if existing_edge_data.get('weight', 0) < confidence: # type: ignore
                         existing_edge_data['weight'] = confidence # type: ignore
                         existing_edge_data['hsp_source_info'] = {'origin_fact_id': hsp_fact_internal_ref_id, 'source_ai': source_ai_id} # type: ignore
                         existing_edge_data['original_predicate_uri'] = p_original_uri # Update if new predicate led to same mapped type # type: ignore
                         edge_added_or_updated = True
                         print(f"ContentAnalyzerModule: Updated existing edge {s_id} -[{p_type}]-> {o_id} with higher confidence/new source.")
                     else:
                        print(f"ContentAnalyzerModule: Edge from triple {s_id} -[{p_type}]-> {o_id} already exists with similar/higher confidence.") # type: ignore

                if self.graph.number_of_nodes() > initial_node_count or edge_added_or_updated:
                    result["updated_graph"] = True

                if result["updated_graph"]: # This means either a node was added or edge was added/updated
                    result["processed_triple"] = ProcessedTripleInfo(
                        subject_id=s_id,
                        predicate_type=p_type,
                        object_id=o_id,
                        original_subject_uri=s_original_uri,
                        original_predicate_uri=p_original_uri,
                        original_object_uri_or_literal=o_original_uri if obj_is_uri else obj_data,
                        object_is_uri=obj_is_uri
                    )

        if result["updated_graph"]:
            print(f"ContentAnalyzerModule: Main graph now has {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges after processing HSP fact.")

        return result

# Example usage:
if __name__ == '__main__':
    analyzer = ContentAnalyzerModule()

    example_text = (
        "Apple Inc. is looking at buying U.K. startup VisionCorp for $1 billion. "
        "Steve Jobs co-founded Apple in Cupertino. Microsoft, another tech giant, also develops software."
        "The new iPhone was announced by Tim Cook. Apples are healthy fruits."
    )

    print(f"\nAnalyzing text: \"{example_text[:100]}...\"")
    kg_data, nx_g = analyzer.analyze_content(example_text) # Get both results

    print("\n--- Extracted Knowledge Graph Data (TypedDict) ---")
    print(f"Metadata: {kg_data['metadata']}")

    print("\nEntities (from TypedDict):")
    for entity_id, entity in kg_data["entities"].items():
        print(f"  ID: {entity_id}, Label: {entity['label']}, Type: {entity['type']}, Attrs: {entity['attributes']}")

    print("\nRelationships (from TypedDict):")
    if kg_data["relationships"]:
        for rel in kg_data["relationships"]:
            source_label = kg_data["entities"].get(rel['source_id'], {}).get('label', rel['source_id'])
            target_label = kg_data["entities"].get(rel['target_id'], {}).get('label', rel['target_id'])
            print(f"  {source_label} --[{rel['type']}(w:{rel.get('weight')})]--> {target_label} (Attrs: {rel['attributes']})")
    else:
        print("  No relationships extracted by this prototype.")

    # --- NetworkX Graph Query Examples (using the first example's graph nx_g) ---
    if nx_g.number_of_nodes() > 0:
        print("\n--- NetworkX Graph Query Examples ---")

        # Find a specific node to query, e.g., first ORG entity if available
        apple_node_id = None
        for node_id, data in nx_g.nodes(data=True):
            if data.get("label") == "Apple Inc.": # More specific than just 'Apple'
                apple_node_id = node_id
                break
            elif data.get("type") == "ORG" and "apple" in data.get("label","").lower() and not apple_node_id: # Fallback
                apple_node_id = node_id


        if apple_node_id:
            print(f"\nQuerying around node: '{nx_g.nodes[apple_node_id].get('label', apple_node_id)}' (ID: {apple_node_id})")

            # 1. Finding neighbors
            print("  Neighbors (successors):")
            for successor in nx_g.successors(apple_node_id):
                print(f"    - {nx_g.nodes[successor].get('label', successor)} (Rel: {nx_g.edges[apple_node_id, successor].get('type')})")
            print("  Neighbors (predecessors):")
            for predecessor in nx_g.predecessors(apple_node_id):
                 print(f"    - {nx_g.nodes[predecessor].get('label', predecessor)} (Rel: {nx_g.edges[predecessor, apple_node_id].get('type')})")

            # 2. Finding a path (example: Apple Inc. to U.K.)
            uk_node_id = None
            for node_id, data in nx_g.nodes(data=True):
                if data.get("label") == "U.K.":
                    uk_node_id = node_id
                    break

            if uk_node_id and nx.has_path(nx_g, source=apple_node_id, target=uk_node_id):
                print(f"\n  Shortest path from '{nx_g.nodes[apple_node_id]['label']}' to '{nx_g.nodes[uk_node_id]['label']}':")
                path = nx.shortest_path(nx_g, source=apple_node_id, target=uk_node_id)
                path_details = []
                for i in range(len(path) - 1):
                    u, v = path[i], path[i+1]
                    edge_data = nx_g.get_edge_data(u,v)
                    path_details.append(f"{nx_g.nodes[u]['label']} -[{edge_data.get('type', 'related')}]-> {nx_g.nodes[v]['label']}")
                print(f"    {' --> '.join(path_details)}")
            elif uk_node_id:
                print(f"\n  No path found from '{nx_g.nodes[apple_node_id]['label']}' to '{nx_g.nodes[uk_node_id]['label']}'.")

        else:
            print("  Could not find 'Apple Inc.' or similar ORG node for detailed queries in the first example.")

        # 3. Filtering nodes by attribute
        print("\n  Filtering nodes (type 'PERSON'):")
        person_nodes = [data['label'] for node, data in nx_g.nodes(data=True) if data.get('type') == 'PERSON']
        if person_nodes:
            for person_label in person_nodes:
                print(f"    - {person_label}")
        else:
            print("    No PERSON entities found.")

        # 4. Filtering edges by attribute
        print("\n  Filtering edges (type 'look_at'):") # Assuming 'look_at' is a possible verb lemma
        look_at_edges = [(nx_g.nodes[u]['label'], nx_g.nodes[v]['label'], data['type'])
                           for u, v, data in nx_g.edges(data=True) if data.get('type') == 'look_at'] # or .get('type','').startswith('look')
        if look_at_edges:
            for u_label, v_label, rel_type in look_at_edges:
                print(f"    - {u_label} --[{rel_type}]--> {v_label}")
        else:
            print("    No 'look_at' relationships found.")
    else:
        print("\nNetworkX graph is empty for the first example, skipping query demos.")


    print("\n--- Second Example ---")
    example_text_2 = "The cat sat on the mat. A dog chased the cat."
    print(f"\nAnalyzing text: \"{example_text_2}\"")
    kg_data_2, nx_g2 = analyzer.analyze_content(example_text_2) # Capture nx_g2
    print("\nEntities (from TypedDict):")
    for entity_id, entity in kg_data_2["entities"].items():
        print(f"  ID: {entity_id}, Label: {entity['label']}, Type: {entity['type']}, Attrs: {entity['attributes']}")
    print("\nRelationships (from TypedDict):")
    if kg_data_2["relationships"]:
        for rel in kg_data_2["relationships"]:
            source_label = kg_data_2["entities"].get(rel['source_id'], {}).get('label', rel['source_id'])
            target_label = kg_data_2["entities"].get(rel['target_id'], {}).get('label', rel['target_id'])
            print(f"  {source_label} --[{rel['type']}(w:{rel.get('weight')})]--> {target_label} (Attrs: {rel['attributes']})")
    else:
        print("  No relationships extracted by this prototype for the second example.")

    # Example showing how it might handle no entities
    print("\n--- Third Example (No clear named entities for sm model) ---")
    example_text_3 = "The weather is nice today."
    print(f"\nAnalyzing text: \"{example_text_3}\"")
    kg_data_3 = analyzer.analyze_content(example_text_3)
    print("\nEntities:")
    if not kg_data_3["entities"]:
        print("  No entities extracted.")
    else:
        for entity_id, entity in kg_data_3["entities"].items():
            print(f"  ID: {entity_id}, Label: {entity['label']}, Type: {entity['type']}, Attrs: {entity['attributes']}")
    print("\nRelationships:")
    if not kg_data_3["relationships"]:
        print("  No relationships extracted.")
    else:
        # ... (print relationships)
        pass
    print("\nContent Analyzer Module example finished.")

# Add __init__.py for the learning directory if it doesn't exist
# This is usually handled by project setup, but good to ensure for module resolution.
# (No tool to create __init__.py directly, will do it if ls shows it's missing)

# Add spacy to requirements.txt
# (Will do this in a separate step)
