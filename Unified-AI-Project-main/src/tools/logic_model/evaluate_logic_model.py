import json
import os
import sys
import numpy as np
import tensorflow as tf
from sklearn.metrics import classification_report, accuracy_score

# Add src directory to sys.path
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, "..", "..", ".."))
SRC_DIR = os.path.join(PROJECT_ROOT, "src")
if SRC_DIR not in sys.path:
    sys.path.insert(0, SRC_DIR)

try:
    from tools.logic_model.logic_model_nn import LogicNNModel, preprocess_logic_data # Assuming load_model is part of LogicNNModel
except ImportError as e:
    print(f"Error importing from logic_model_nn: {e}")
    sys.exit(1)

# --- Configuration ---
TEST_DATA_PATH = os.path.join(PROJECT_ROOT, "data/raw_datasets/logic_test.json")
MODEL_LOAD_PATH = os.path.join(PROJECT_ROOT, "data/models/logic_model_nn.keras")
CHAR_MAP_LOAD_PATH = os.path.join(PROJECT_ROOT, "data/models/logic_model_char_maps.json")

def load_logic_test_dataset(file_path):
    """Loads the logic test dataset from a JSON file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        if not isinstance(dataset, list) or \
           not all(isinstance(item, dict) and "proposition" in item and "answer" in item for item in dataset):
            raise ValueError("Test dataset format is incorrect.")
        return dataset
    except FileNotFoundError:
        print(f"Error: Test dataset file not found at {file_path}")
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {file_path}")
    except ValueError as e:
        print(f"Error: {e}")
    return None

def main():
    print("Starting Logic NN Model evaluation process...")

    # 1. Load character maps (needed for model loading and preprocessing)
    print(f"Loading character maps from {CHAR_MAP_LOAD_PATH}...")
    try:
        with open(CHAR_MAP_LOAD_PATH, 'r') as f:
            char_maps = json.load(f)
        char_to_token = char_maps['char_to_token']
        max_seq_len = char_maps['max_seq_len']
        # vocab_size = char_maps['vocab_size'] # Needed if building model structure here
    except FileNotFoundError:
        print(f"Error: Character map file not found at {CHAR_MAP_LOAD_PATH}. Cannot proceed.")
        return
    except Exception as e:
        print(f"Error loading character maps: {e}. Cannot proceed.")
        return
    print("Character maps loaded.")

    # 2. Load the trained model
    print(f"Loading trained model from {MODEL_LOAD_PATH}...")
    try:
        # LogicNNModel.load_model also needs embedding_dim and lstm_units if they are not in char_maps
        # These should ideally be saved with the model or char_maps.
        # Using values from train_logic_model.py for now.
        embedding_dim_eval = 32
        lstm_units_eval = 64

        # The load_model method in LogicNNModel handles rebuilding the structure with these params
        # and then loading weights.
        logic_nn_model_instance = LogicNNModel.load_model(MODEL_LOAD_PATH, CHAR_MAP_LOAD_PATH)
        # If load_model needs explicit dims:
        # logic_nn_model_instance = LogicNNModel.load_model(MODEL_LOAD_PATH, CHAR_MAP_LOAD_PATH,
        #                                                 embedding_dim=embedding_dim_eval,
        #                                                 lstm_units=lstm_units_eval)
        if logic_nn_model_instance is None or logic_nn_model_instance.model is None:
             print("Failed to load model structure properly.")
             return
        print("Model loaded successfully.")
    except FileNotFoundError:
        print(f"Error: Model file not found at {MODEL_LOAD_PATH}. Train the model first.")
        return
    except Exception as e:
        print(f"An error occurred while loading the model: {e}")
        return

    # 3. Load test data
    print(f"Loading test dataset from {TEST_DATA_PATH}...")
    test_dataset = load_logic_test_dataset(TEST_DATA_PATH)
    if test_dataset is None:
        return
    print(f"Loaded {len(test_dataset)} test samples.")

    # 4. Preprocess test data
    print("Preprocessing test data...")
    # Extract propositions and actual answers (booleans)
    test_propositions_str = [item['proposition'] for item in test_dataset]
    test_answers_bool = [item['answer'] for item in test_dataset]

    # Tokenize and pad propositions
    sequences = [[char_to_token.get(char, char_to_token['<UNK>']) for char in prop] for prop in test_propositions_str]
    X_test = pad_sequences(sequences, maxlen=max_seq_len, padding='post', value=char_to_token['<PAD>'])

    # True answers as integers (0 or 1) for scikit-learn metrics
    y_test_true_int = np.array([1 if ans else 0 for ans in test_answers_bool])

    print(f"X_test (input data) shape: {X_test.shape}")

    # 5. Make predictions
    print("Making predictions on the test set...")
    predictions_proba = logic_nn_model_instance.model.predict(X_test, verbose=0)
    y_test_pred_int = np.argmax(predictions_proba, axis=1) # Get class with highest probability

    # 6. Evaluate and report
    accuracy = accuracy_score(y_test_true_int, y_test_pred_int)
    print(f"\n--- Evaluation Results ---")
    print(f"Accuracy: {accuracy*100:.2f}%")

    print("\nClassification Report:")
    # target_names should correspond to how classes are indexed (e.g., 0: False, 1: True)
    print(classification_report(y_test_true_int, y_test_pred_int, target_names=['False', 'True'], zero_division_report=0))

    print("\nSample Predictions (first 5):")
    for i in range(min(5, len(test_dataset))):
        prop_str = test_propositions_str[i]
        true_ans = test_answers_bool[i]
        pred_ans_int = y_test_pred_int[i]
        pred_ans_bool = bool(pred_ans_int)
        print(f"  Proposition: \"{prop_str}\"")
        print(f"  Expected: {true_ans}, Got: {pred_ans_bool} ({'CORRECT' if true_ans == pred_ans_bool else 'INCORRECT'})")
        print("  ---")

    print("Evaluation process complete.")

if __name__ == '__main__':
    # Ensure necessary files exist
    files_ok = True
    if not os.path.exists(MODEL_LOAD_PATH):
        print(f"Model file not found: {MODEL_LOAD_PATH}")
        files_ok = False
    if not os.path.exists(CHAR_MAP_LOAD_PATH):
        print(f"Character map file not found: {CHAR_MAP_LOAD_PATH}")
        files_ok = False
    if not os.path.exists(TEST_DATA_PATH):
        print(f"Test data file not found: {TEST_DATA_PATH}")
        print("Please run `logic_data_generator.py` to create 'logic_test.json'.")
        files_ok = False

    if files_ok:
        main()
    else:
        print("Evaluation cannot proceed due to missing files. Please train the model and generate test data first.")
