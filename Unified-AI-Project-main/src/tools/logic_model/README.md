# Lightweight Logic Model

This directory contains components for a lightweight system designed to evaluate simple logical propositions (e.g., "true AND (NOT false OR true)"). It offers two evaluation methods: a parser-based evaluator and a neural network-based evaluator.

## Overview

The Logic Model aims to provide a basic reasoning capability to MikoAI by enabling the evaluation of Boolean expressions.

### Components:

1.  **`logic_data_generator.py`**:
    *   Generates datasets of logical propositions and their corresponding boolean answers.
    *   Outputs data in JSON format, suitable for training the neural network model or for testing either evaluator.
    *   Supports generating expressions with `AND`, `OR`, `NOT`, `true`, `false`, and parentheses, with configurable nesting depth.

2.  **`logic_parser_eval.py`**:
    *   Implements the `LogicParserEval` class.
    *   This is a deterministic, rule-based parser and evaluator for the defined logical expressions.
    *   It tokenizes the input string and uses a recursive descent approach to evaluate the expression, respecting operator precedence (NOT > AND > OR) and parentheses.
    *   **Recommended for reliable and accurate evaluation of well-formed simple expressions.**

3.  **`logic_model_nn.py`**:
    *   Implements the `LogicNNModel` class, a sequence-to-sequence (specifically, sequence-to-category for this task) neural network using TensorFlow/Keras.
    *   Architecture: Embedding layer -> LSTM layer -> Dense output layer (softmax for True/False classification).
    *   Includes helper functions for creating character-to-token maps (`get_logic_char_token_maps`) and preprocessing data (`preprocess_logic_data`).
    *   This model learns to evaluate expressions from examples. Its performance depends on the training data and hyperparameters.

4.  **`train_logic_model.py`**:
    *   Script to train the `LogicNNModel`.
    *   Loads training data generated by `logic_data_generator.py`.
    *   Preprocesses the data, builds the model, and runs the training loop.
    *   Saves the trained model weights (e.g., to `data/models/logic_model_nn.keras`) and the character maps used during training (`logic_model_char_maps.json`).

5.  **`evaluate_logic_model.py`**:
    *   Script to evaluate a trained `LogicNNModel`.
    *   Loads a test dataset, the trained model, and character maps.
    *   Reports accuracy and other classification metrics.

6.  **`logic_tool.py`** (located in the parent `tools` directory):
    *   Provides a unified interface function `evaluate_expression(expression_string, method='parser')`.
    *   This function can call either the `LogicParserEval` or the trained `LogicNNModel` based on the `method` argument.
    *   It's intended to be used by the main `ToolDispatcher`.

## Setup

1.  **Environment**: Ensure Python 3.x is installed. For the neural network model, TensorFlow is required. Other dependencies like `scikit-learn` (for `classification_report` in `evaluate_logic_model.py`) might be needed.
    ```bash
    pip install tensorflow numpy scikit-learn
    ```
    (Manage dependencies at the project root, e.g., via `requirements.txt`).

## Usage Workflow

### 1. Data Generation

Generate training and test datasets for the logic model evaluators.
```bash
python src/tools/logic_model/logic_data_generator.py
```
This will create:
-   `data/raw_datasets/logic_train.json`
-   `data/raw_datasets/logic_test.json`

### 2. Training the Neural Network Model (Optional)

If you intend to use the neural network evaluator:
```bash
python src/tools/logic_model/train_logic_model.py
```
This will train the model and save its weights to `data/models/logic_model_nn.keras` and character maps to `logic_model_char_maps.json`.

### 3. Evaluating the Neural Network Model (Optional)

After training the NN model:
```bash
python src/tools/logic_model/evaluate_logic_model.py
```
This will output performance metrics for the NN model on the test set.

### 4. Using the Logic Tool for Evaluation

The primary way to use the logic evaluation capability is through `logic_tool.py`, which can be invoked by the `ToolDispatcher` or used directly for testing.

**Directly testing `logic_tool.py`:**
```bash
python src/tools/logic_tool.py
```
This runs the `if __name__ == '__main__':` block in `logic_tool.py`, which demonstrates calling `evaluate_expression` with both `'parser'` and `'nn'` methods.

**Via ToolDispatcher:**
The `ToolDispatcher` (in `src/tools/tool_dispatcher.py`) is designed to infer when a query requires logical evaluation and will call `logic_tool.evaluate_expression`.
Example query for the dispatcher: `"evaluate true AND (NOT false)"` or `"logic: false OR true"`.

## Parser vs. Neural Network Model

*   **Parser-based (`LogicParserEval`):**
    *   **Pros:** Deterministic, 100% accurate for well-defined grammar, fast for simple expressions, no training required.
    *   **Cons:** Less flexible to variations in input format, does not handle ambiguity or "fuzzy" logic, requires explicit grammar definition.
    *   **Recommended for:** Reliable evaluation of standard logical propositions. This is the default method in `logic_tool.py`.

*   **Neural Network-based (`LogicNNModel`):**
    *   **Pros:** Can potentially learn to handle more varied or slightly malformed inputs (if trained on such data), can learn complex patterns if the problem space were larger.
    *   **Cons:** Requires training data, training time, and computational resources. Performance is not guaranteed to be 100% and depends on data quality and model capacity. For simple Boolean logic, it's often an overly complex solution compared to a parser.
    *   **Recommended for:** Experimental purposes, or if the scope of "logic" expands to areas where rule-based parsing is difficult (e.g., natural language based logical reasoning, fuzzy logic - though the current model is not designed for this).

For the defined scope of simple Boolean expressions, the **parser-based evaluator is generally preferred for its accuracy and simplicity.** The NN model is provided as an alternative approach and a learning example.

## Future Enhancements
*   More robust parsing in `LogicParserEval` (e.g., using a proper parsing library for more complex grammars).
*   Training the `LogicNNModel` on a larger and more diverse dataset, including slightly malformed inputs.
*   Extending the supported logical operators and functions.
*   Improving error reporting for invalid expressions.
*   **Current Status:** Tests currently show failures indicating instability in core model components. Work is ongoing to improve reliability and functionality.
